{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "07090ff2-4210-4a37-8692-52959d581d7d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Bronze layer\n",
    "Purpose of this layer is to be a source of truth (raw data) and also make sure my information and data is presented in a suitable data structure\n",
    "\n",
    "Exercise is fairly straight forward and simple, so I will not use metastore, path based delta is sufficient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc6bbdc4-52ba-44b1-8054-d87213b50bb7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Sensor data pre-processing\n",
    "\n",
    "Sensor 1: XLSX (use binary + pandas)\n",
    "\n",
    "\n",
    "Sensor 2: CSV (supported)\n",
    "\n",
    "\n",
    "Sensor 4: PARQUET (supported) + PICKLE (use binary + pandas)\n",
    "\n",
    "\n",
    "Sensor 5: JSON (supported, with UDF + schema)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "accb9377-e782-4d0c-a6a5-4b52aac8bcde",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1176b96b-ff90-4144-820d-a3c4c8e2d1d5",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Import"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, explode, udf\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, ArrayType, MapType\n",
    "\n",
    "import json\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "from io import BytesIO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "19e1b387-f1ed-4fde-83bc-230510325fde",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "### Schema\n",
    "\n",
    "Can use shared explicit schema because all data sets share the same structure:\n",
    "1. Created timestamp\n",
    "2. Tag Key\n",
    "3. Tag Value\n",
    "4. Tag Quality\n",
    "\n",
    "Also helps with union if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3121884f-8091-4b57-b251-81e0d55eb523",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sensor_schema = StructType([\n",
    "    StructField(\"created_timestamp\", StringType(), True),\n",
    "    StructField(\"tag_key\", StringType(), True),\n",
    "    StructField(\"tag_val\", DoubleType(), True),\n",
    "    StructField(\"tag_quality\", StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "67829d93-d66a-4251-9bcf-08591c78642c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "### Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c280088-1bf5-4087-a5be-2a8b0f2524ce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "source = \"abfss://source@sembcorpete.dfs.core.windows.net\"\n",
    "bronze = \"abfss://bronze@sembcorpete.dfs.core.windows.net\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa0ed7e6-42e1-4175-a076-059cd15b5a93",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "### Sensor 1 read\n",
    "\n",
    "XLSX not supported natively by Spark, so have to use spark-excel library\n",
    "\n",
    "NO ETL + ELT, unified source of truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "da515cf2-8ea5-4cdf-b0e9-e1ec59388d08",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    import openpyxl\n",
    "except ImportError:\n",
    "    %pip install openpyxl\n",
    "    dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33615e71-33db-46cb-ba41-2e5defd96acb",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Sensor 1"
    }
   },
   "outputs": [],
   "source": [
    "#spark excel library read\n",
    "bronze_sensor1_df = spark.read \\\n",
    "                      .format(\"com.crealytics.spark.excel\") \\\n",
    "                      .option(\"sheetName\", \"Sheet1\") \\\n",
    "                      .option(\"header\", \"true\") \\\n",
    "                      .schema(sensor_schema) \\\n",
    "                      .load(f\"{source}/sensor1/sensor_1.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ed3b4534-1f28-4b02-aa3b-32aae9022214",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#save to bronze for silver use, schema evolution with mergeSchema\n",
    "bronze_sensor1_df.write \\\n",
    "                 .mode(\"overwrite\") \\\n",
    "                 .format(\"delta\") \\\n",
    "                 .option(\"mergeSchema\", \"true\").save(f\"{bronze}/sensor2/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f0e23dc0-6693-434f-8193-6ed96cac9290",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "### Sensor 2 read\n",
    "**CSV** format supported in spark, batch read file\n",
    "\n",
    "Since we know the file exists because of Azure set-up, we can just use a simple read\n",
    "\n",
    "NO ETL + ELT, unified source of truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "74324280-07ba-406c-9d13-2f0c030f277c",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Sensor 2"
    }
   },
   "outputs": [],
   "source": [
    "bronze_sensor2_df = spark.read \\\n",
    "                         .format('csv') \\\n",
    "                         .option(\"header\", \"true\") \\\n",
    "                         .schema(sensor_schema) \\\n",
    "                         .load(f\"{source}/sensor2/\")\n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb0da41d-6e9b-4a31-9f58-9a5181e4ae5c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#save to bronze for silver use, schema evolution with mergeSchema\n",
    "bronze_sensor2_df.write \\\n",
    "                 .mode(\"overwrite\") \\\n",
    "                 .format(\"delta\") \\\n",
    "                 .option(\"mergeSchema\", \"true\").save(f\"{bronze}/sensor2/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fa2e2d9b-02d2-4f5b-8df6-3d81c6f05ac5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "### Sensor 4 read\n",
    "**PARQUET** format supported, batch read sensor 4 parquet\n",
    "\n",
    "**PICKLE** format not supported: Can use UDF (AI suggested, but slower processing due to UDF), so I approach this using Pandas (will be limited if file size is too big for pandas).\n",
    "\n",
    "**Need to approach using binary file read for pickle**\n",
    "\n",
    "\n",
    " Alternatively, since we are doing batch ingestion, we can also convert file to parquet and pass the new file into azure, then use spark to process and read the parquet files!\n",
    "\n",
    "We also save to 2 different folders in bronze container to prevent overwriting each other + cleaner design \n",
    "\n",
    "\n",
    " NO ETL + ELT, unified source of truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "89f9b525-9be5-4bd4-96e0-fda06873b3fb",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Sensor 4 (parquet)"
    }
   },
   "outputs": [],
   "source": [
    "#sensor 4 parquet read\n",
    "bronze_sensor4_df = spark.read \\\n",
    "                  .format('PARQUET') \\\n",
    "                  .option(\"header\", \"true\") \\\n",
    "                  .schema(sensor_schema) \\\n",
    "                  .load(f\"{source}/sensor4/sensor_4.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f6fa6d46-a8ee-4c70-a439-facf9da1d223",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#sensor 4 parquet write\n",
    "bronze_sensor4_df.write \\\n",
    "                 .mode(\"overwrite\") \\\n",
    "                 .format(\"delta\") \\\n",
    "                 .option(\"mergeSchema\", \"true\").save(f\"{bronze}/sensor4/parquet/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a63dbafd-15b1-41b6-8a11-a151fde254cc",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Sensor 4 (Pickle)"
    }
   },
   "outputs": [],
   "source": [
    "#sensor 4 pickle read\n",
    "binary_df = spark.read \\\n",
    "                 .format(\"binaryFile\") \\\n",
    "                 .load(f\"{source}/sensor4/sensor_4_diff_date.pickle\")\n",
    "\n",
    "int_processing = binary_df.select(\"content\").collect()[0][0]\n",
    "\n",
    "pandas_df = pickle.loads(int_processing)\n",
    "bronze_sensor4_dd_df = spark.createDataFrame(pandas_df, schema=sensor_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae1610fa-bdb9-436d-85e1-240c2036a6e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#sensor 4 pickle write\n",
    "bronze_sensor4_dd_df.write \\\n",
    "                 .mode(\"overwrite\") \\\n",
    "                 .format(\"delta\") \\\n",
    "                 .option(\"mergeSchema\", \"true\").save(f\"{bronze}/sensor4/pickle/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "947591d0-4605-44db-a744-569377e51da4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "### Sensor 5 read\n",
    "**JSON** files supported so processing is generally straight forward\n",
    "\n",
    "However with JSON files, we need to pay attention to Schema\n",
    "\n",
    "\n",
    "so we use STRUCT columns:\n",
    "\n",
    "\n",
    "1. Schema enforcement\n",
    "\n",
    "\n",
    "2. Improved performance\n",
    "\n",
    " NO ETL + ELT, unified source of truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "30af6b30-6820-4396-9ae6-2d7caf45c748",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Sensor 5"
    }
   },
   "outputs": [],
   "source": [
    "#basic udf logic from my lecture notes, can use variant json function in databricks (need to handle variant schema)\n",
    "def parse_json(json_str):\n",
    "    t = type(json_str)\n",
    "    if json_str:\n",
    "        if t == str:\n",
    "            json_obj = json.loads(json_str)\n",
    "        else:\n",
    "            json_obj = json_str\n",
    "\n",
    "        sensor_array = []\n",
    "        for key, item in json_obj.items():\n",
    "            created_timestamp = item.get(\"created_timestamp\")\n",
    "            tag_key = item.get(\"tag_key\")\n",
    "            tag_val = item.get(\"tag_val\")\n",
    "            tag_quality = item.get(\"tag_quality\")\n",
    "            sensor_array.append((created_timestamp, tag_key, tag_val, tag_quality))\n",
    "        return sensor_array\n",
    "\n",
    "json_sensor_schema = ArrayType(sensor_schema)\n",
    "udf_parse_json = udf(parse_json, json_sensor_schema)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eebbcc0c-ef1a-4124-8539-3ee0899cb9df",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "bronze_sensor5_df = spark.read \\\n",
    "                      .format(\"text\") \\\n",
    "                      .option(\"wholetext\", \"true\") \\\n",
    "                      .load(f\"{source}/sensor5/\") \\\n",
    "                      .selectExpr(\"CAST(value AS STRING) as raw_json\") \\\n",
    "                      .withColumn(\"parsed_records\", udf_parse_json(col(\"raw_json\"))) \\\n",
    "                      .select(explode(col(\"parsed_records\")).alias(\"record\")) \\\n",
    "                      .select(\n",
    "                          col(\"record.created_timestamp\").alias(\"created_timestamp\"),\n",
    "                          col(\"record.tag_key\").alias(\"tag_key\"),\n",
    "                          col(\"record.tag_val\").alias(\"tag_val\"),\n",
    "                          col(\"record.tag_quality\").alias(\"tag_quality\")\n",
    "                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "24238c71-f3a5-4f7f-913a-6b75cf5fd602",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "bronze_sensor5_df.write \\\n",
    "                 .mode(\"overwrite\") \\\n",
    "                 .format(\"delta\") \\\n",
    "                 .option(\"mergeSchema\", \"true\").save(f\"{bronze}/sensor5\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Bronze_layer",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
