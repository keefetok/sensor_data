{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "221f5ec1-ac8e-4a80-bb47-b75e6823875b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Gold Layer\n",
    "Purpose of this layer is for feature engineering in our scenario (or business case use)\n",
    "\n",
    "In silver layer, we already transformed and processed data (filter, pivot, forward fill to fix missing timestamps)\n",
    "\n",
    "\n",
    "Now in this layer we have the necessary information we need to build data normalisation for each sensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "edbe266f-4d50-4c29-96b6-fb3b89d671f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f6acb557-3fba-4524-a503-71c5da8beb96",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import MinMaxScaler, VectorAssembler\n",
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "from pyspark.ml.functions import vector_to_array\n",
    "\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import col, first, last, to_timestamp, lower\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, ArrayType, MapType\n",
    "\n",
    "import json\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "from io import BytesIO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9bc1525a-e8f0-4cce-8265-a134f9e93952",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "### Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e0462fd-30be-4420-906b-9cdfd7464c5f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "silver_source = \"abfss://silver@sembcorpete.dfs.core.windows.net/\"\n",
    "gold = \"abfss://gold@sembcorpete.dfs.core.windows.net/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a052b651-2431-4418-bf03-f02dae5022a3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1e08959f-dd44-423a-b606-46fa29452095",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def build_normalization_pipeline(sensor_cols):\n",
    "    \"\"\"\n",
    "    Min Max Scaler for data normalisation \n",
    "    \n",
    "    Args:\n",
    "        sensor_cols: List of sensor column names to normalise \n",
    "    \n",
    "    Returns:\n",
    "        Pipeline, list of (scaled_col, normalised_col) for us to extract value from\n",
    "    \"\"\"\n",
    "\n",
    "    assemblers = []\n",
    "    scalers = []\n",
    "    normalised_cols = []\n",
    "\n",
    "    \n",
    "    for sensor in sensor_cols:\n",
    "        #vector column for each sensor\n",
    "        vec_col = f\"{sensor}_vec\"\n",
    "        scaled_col = f\"{sensor}_scaled_vec\"\n",
    "        normalised_col = f\"{sensor}_normalized\"\n",
    "        \n",
    "        #assembler to convert single column to vector\n",
    "        assembler = VectorAssembler(\n",
    "            inputCols=[sensor],\n",
    "            outputCol=vec_col,\n",
    "            handleInvalid=\"keep\"\n",
    "        )\n",
    "    \n",
    "        \n",
    "        #scale to 0, 1\n",
    "        scaler = MinMaxScaler(\n",
    "            inputCol=vec_col,\n",
    "            outputCol=scaled_col,\n",
    "            min=0.0,\n",
    "            max=1.0\n",
    "        )\n",
    "        \n",
    "        assemblers.append(assembler)\n",
    "        scalers.append(scaler)\n",
    "        normalised_cols.append((scaled_col, normalised_col))\n",
    "\n",
    "    # build pipeline stages\n",
    "    pipeline_stages = []\n",
    "    for assembler, scaler in zip(assemblers, scalers):\n",
    "        pipeline_stages.extend([assembler, scaler])\n",
    "    \n",
    "    pipeline = Pipeline(stages=pipeline_stages)\n",
    "    \n",
    "    return pipeline, normalised_cols\n",
    "\n",
    "\n",
    "def fit_and_transform(pipeline, df, normalised_cols):\n",
    "    \"\"\"\n",
    "    Fit pipeline and transform data\n",
    "    \n",
    "    Returns:\n",
    "        fitted_model, transformed_df\n",
    "    \"\"\"\n",
    "    # Fit model\n",
    "    model = pipeline.fit(df)\n",
    "    \n",
    "    # Transform\n",
    "    transformed = model.transform(df)\n",
    "    \n",
    "    # Extract normalized values from vectors\n",
    "    for scaled_col, normalized_col in normalised_cols:\n",
    "        transformed = transformed.withColumn(\n",
    "            normalised_col,\n",
    "            vector_to_array(col(scaled_col))[0]\n",
    "        )\n",
    "    \n",
    "    return model, transformed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ab19f6f-f04c-4232-9ffb-07097eef25b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Read from silver container before building data normalisation\n",
    "\n",
    "Requirement is to normalise to range 0 to 1 for each sensor\n",
    "\n",
    "If we do a union of all sets and process, it will result in NULL values for dates that are earlier than some set's dates. This is because fill forward has a limitation where only if data exists, then it can fill forward\n",
    "\n",
    "   i.e Sensors 1/2 has dates only on Jul 5th. If we include sensor 5 with sensors 1/2 then there will be no fill forward for sensors 1/2, only NULL values\n",
    "\n",
    "   i.e Sensor 1 does not have values in June, so cannot fill forward\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "85285c2f-c624-41ab-a4dc-cfeb9c8a8a9c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Set with all sensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "576bc5e8-65a9-4ef2-ae95-641344a917fe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "all_silver_ffilled = spark.read \\\n",
    "                          .format(\"delta\") \\\n",
    "                          .load(f\"{silver_source}/ffill/all_sensors_ffill/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "92052e19-a6d3-4dca-a310-7370110053d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## All sensor union pipeline + modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d6595a6-b01b-48a3-95fb-6332ac982fff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#get all sensors\n",
    "sensor_cols = [c for c in all_silver_ffilled.columns if c != \"timestamp\"]\n",
    "\n",
    "#build pipeline\n",
    "pipeline_all, normalized_cols_all = build_normalization_pipeline(sensor_cols)\n",
    "\n",
    "#fit and transform\n",
    "normalisation_model_all, gold_normalised_all = fit_and_transform(\n",
    "    pipeline_all, \n",
    "    all_silver_ffilled, \n",
    "    normalized_cols_all\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c31db412-6442-4e16-a8d5-49d434e423c5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "final_cols = [\"timestamp\"] + sensor_cols + [nc for _, nc in normalised_cols]\n",
    "gold_final = gold_normalised.select(final_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "98b8eba6-4608-4786-addf-f7e174ccb2c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Reusability + saving DF and models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd6178a4-7724-4628-b758-af051a731e88",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "normalisation_model.write() \\\n",
    "                       .overwrite() \\\n",
    "                       .save(f\"{gold}/all_sensors/models/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e40ec5a7-f136-4499-869e-cb9c6d89ec76",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "gold_final.write \\\n",
    "          .mode(\"overwrite\") \\\n",
    "          .format(\"delta\") \\\n",
    "          .option(\"mergeSchema\", \"true\").save(f\"{gold}/all_sensors/tables/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d82cfd7-a3b0-4d1f-b4b6-6fd38cfb5f52",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "#### Business level data, we minimise null values from fill forward limitation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f0a6dd1-afe6-46e7-aacc-0b21fe78254c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "gold_sensor1 = gold_final.select(\"timestamp\", \n",
    "                                 \"sens_1\", \n",
    "                                 \"sens_1_normalized\").filter(col(\"sens_1\").isNotNull())\n",
    "\n",
    "gold_sensor2 = gold_final.select(\"timestamp\", \n",
    "                                 \"sens_2\", \n",
    "                                 \"sens_2_normalized\").filter(col(\"sens_2\").isNotNull())\n",
    "\n",
    "gold_sensor4 = gold_final.select(\"timestamp\", \n",
    "                                 \"sens_4\", \n",
    "                                 \"sens_4_normalized\").filter(col(\"sens_4\").isNotNull())\n",
    "\n",
    "gold_sensor5 = gold_final.select(\"timestamp\", \n",
    "                                 \"sens_5\", \n",
    "                                 \"sens_5_normalized\").filter(col(\"sens_5\").isNotNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dee1ffef-9da5-4496-b7bc-771a0b56fadf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "gold_sensor1.write \\\n",
    "            .mode(\"overwrite\") \\\n",
    "            .format(\"delta\") \\\n",
    "            .option(\"mergeSchema\", \"true\").save(f\"{gold}/sensor1/tables/\")\n",
    "\n",
    "gold_sensor2.write \\\n",
    "            .mode(\"overwrite\") \\\n",
    "            .format(\"delta\") \\\n",
    "            .option(\"mergeSchema\", \"true\").save(f\"{gold}/sensor2/tables/\")\n",
    "\n",
    "gold_sensor4.write \\\n",
    "            .mode(\"overwrite\") \\\n",
    "            .format(\"delta\") \\\n",
    "            .option(\"mergeSchema\", \"true\").save(f\"{gold}/sensor4/tables/\")\n",
    "\n",
    "gold_sensor5.write \\\n",
    "            .mode(\"overwrite\") \\\n",
    "            .format(\"delta\") \\\n",
    "            .option(\"mergeSchema\", \"true\").save(f\"{gold}/sensor5/tables/\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Gold_layer",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
